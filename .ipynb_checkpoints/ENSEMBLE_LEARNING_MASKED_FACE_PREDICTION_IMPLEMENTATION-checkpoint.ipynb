{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af75e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "464b5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "import pathlib\n",
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import  Conv2D, MaxPooling2D, Flatten, Dense,Dropout,BatchNormalization\n",
    "import tensorflow.keras \n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras.utils as utils\n",
    "from tensorflow.keras.optimizers import Adam as adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adagrad \n",
    "from tensorflow.keras.callbacks import  EarlyStopping ,ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import  Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6191cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.4.0\n",
      "TensorFlow version: 2.4.0\n",
      "Python version: 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a623c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd ..\n",
    "def get_paths():\n",
    "    classes = []\n",
    "    for file in sorted(glob.iglob('lfw3/lfw3/lfw-deepfunneled_masked/*/')):\n",
    "        classes.append(file)\n",
    "    for i,d in enumerate(classes):\n",
    "        paths=d+'*.jpg'\n",
    "        class_=[]     \n",
    "        for file in sorted(glob.iglob(paths)):\n",
    "            class_.append(file)\n",
    "        classes[i]=class_ \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8597a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_images(classes):\n",
    "    allImages=[]\n",
    "    Labels = []\n",
    "    count = 0\n",
    "    for k,d in enumerate(classes[:]):\n",
    "        for i,sample in enumerate(d):\n",
    "            org_img = cv2.imread(sample)\n",
    "            #org_img = org_img.astype('float32')\n",
    "            org_img = cv2.resize(org_img, (224, 224))\n",
    "            # org_img=cv2.cvtColor(org_img,cv2.COLOR_BGR2RGB)\n",
    "            # np.append(allImages, org_img)\n",
    "            allImages.append(org_img)\n",
    "            Labels.append(count)\n",
    "        count = count + 1   \n",
    "    return np.array(allImages),np.array(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045b9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=get_paths()\n",
    "X,Y=get_all_images(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6270c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.05,shuffle=True, random_state=777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b4e54",
   "metadata": {},
   "source": [
    "# Transfer_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8742fa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# load the model\n",
    "def Transfer_FacenetModel_withNormlization():\n",
    "    facenetmodel = tf.keras.models.load_model('facenet_keras.h5',compile=False)\n",
    "    facenetmodel.load_weights(\"facenet_keras_weights.h5\")\n",
    "    for layer in facenetmodel.layers[:-50]:\n",
    "        layer.trainable = False\n",
    "    inputs = layers.Input(shape=(224,224,3))\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n",
    "    top_model = facenetmodel(augmented)\n",
    "    top_model = Dropout(0.5)(top_model)\n",
    "    top_model = BatchNormalization()(top_model)\n",
    "    top_model = Flatten(name=\"flatten\")(top_model)\n",
    "    output_layer = Dense(5750, activation='softmax')(top_model)\n",
    "\n",
    "    # Group the convolutional base and new fully-connected layers into a Model object.\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62d3e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Rescaling(scale=1./127.5, offset=-1),\n",
    "        layers.experimental.preprocessing.Resizing(160, 160),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fe98cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Facenet1=Transfer_FacenetModel_withNormlization()\n",
    "Facenet1.load_weights('MyEn3facenet.h5')\n",
    "Facenet2=Transfer_FacenetModel_withNormlization()\n",
    "Facenet2.load_weights('MyEn4facenet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62a31f9",
   "metadata": {},
   "source": [
    "# Transformer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ebbc82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensamble Learning Training\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "weight_decay = 0.0001\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "projection_dim = 64\n",
    "\n",
    "num_heads = 8\n",
    "\n",
    "transformer_units = [\n",
    "\n",
    "    projection_dim * 2,\n",
    "\n",
    "    projection_dim,\n",
    "\n",
    "]  # Size of the transformer layers\n",
    "\n",
    "transformer_layers = 10\n",
    "\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d620c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccd84bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "\n",
    "    [\n",
    "\n",
    "        layers.experimental.preprocessing.Rescaling(1./255),\n",
    "\n",
    "        layers.experimental.preprocessing.Resizing(image_size, image_size),\n",
    "\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\")\n",
    "\n",
    "        #,layers.RandomRotation(0.3)\n",
    "\n",
    "    ],\n",
    "\n",
    "    name=\"data_augmentation\",\n",
    "\n",
    ")\n",
    "\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "\n",
    "#data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8402ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64859e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "\n",
    "    inputs = layers.Input(shape=(224,224,3))\n",
    "\n",
    "    # Augment data.\n",
    "\n",
    "    augmented = data_augmentation(inputs)\n",
    "\n",
    "    # Create patches.\n",
    "\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "\n",
    "    # Encode patches.\n",
    "\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "\n",
    "    for _ in range(transformer_layers):\n",
    "\n",
    "        # Layer normalization 1.\n",
    "\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.3\n",
    "\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "        # MLP.\n",
    "\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.3)\n",
    "\n",
    "        # Skip connection 2.\n",
    "\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    representation = layers.Flatten()(representation)\n",
    "\n",
    "    representation = layers.Dropout(0.6)(representation)\n",
    "\n",
    "    # Add MLP.\n",
    "\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.6)\n",
    "\n",
    "    # Classify outputs.\n",
    "\n",
    "    logits = layers.Dense(5750)(features)\n",
    "\n",
    "    # Create the Keras model.\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "512c6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38232450",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformerClassifier1 = create_vit_classifier()\n",
    "transformerClassifier2 = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "510a0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformerClassifier1.load_weights('FirstTransformer3Ensamble1.h5')\n",
    "transformerClassifier2.load_weights('FirstTransformer3Ensamble2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a241d1",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed1e905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob1=Facenet1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96b4c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob2=Facenet2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd36f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer12 = tf.keras.layers.Softmax()\n",
    "pred_prob3 = layer12(transformerClassifier1.predict(x_test)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1457a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob4 = layer12(transformerClassifier2.predict(x_test)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7dd57",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7496e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def evaluate_model(predictions):\n",
    "    preds_classes = np.argmax(predictions, axis=-1)\n",
    "    accuracy = accuracy_score(y_test, preds_classes)\n",
    "    return accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68f909fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Facenet 1: 80.30418250950571 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Facenet 1:\", evaluate_model(pred_prob1),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2e5c559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Facenet 2: 79.08745247148289 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Facenet 2:\", evaluate_model(pred_prob2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8eab2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Tranformer 1: 69.04942965779468 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Tranformer 1:\", evaluate_model(pred_prob3),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a47d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Tranformer 2: 68.59315589353612 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Tranformer 2:\", evaluate_model(pred_prob4),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b141fc",
   "metadata": {},
   "source": [
    "# Ensemble Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a35f2",
   "metadata": {},
   "source": [
    "## Combining Facenet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "403e2ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Facenet ensemble: 87.9847908745247 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Facenet ensemble:\", evaluate_model(pred_prob1+pred_prob2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807e48f",
   "metadata": {},
   "source": [
    "## Combining Transformer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23a055f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Tranformer ensemble: 79.46768060836501 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Tranformer ensemble:\", evaluate_model(pred_prob3+pred_prob4),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85101b8e",
   "metadata": {},
   "source": [
    "## Combining all models together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "768c896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for complete ensemble: 92.01520912547528 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for complete ensemble:\", evaluate_model(pred_prob1+pred_prob2+pred_prob3+pred_prob4),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b576dd6",
   "metadata": {},
   "source": [
    "# Prediction with varied Test sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b3788",
   "metadata": {},
   "source": [
    "## For test_size = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a59da79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train1, x_test1, y_train, y_test = train_test_split(X,Y, test_size=0.1,shuffle=True, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "222b54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob1=Facenet1.predict(x_test1)\n",
    "pred_prob2=Facenet2.predict(x_test1)\n",
    "pred_prob3=layer12(transformerClassifier1.predict(x_test1)).numpy()\n",
    "pred_prob4=layer12(transformerClassifier2.predict(x_test1)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8b40b8",
   "metadata": {},
   "source": [
    "### Standalone models accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4636215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Facenet 1: 88.09885931558935 %\n",
      "Accuracy for Facenet 2: 87.6045627376426 %\n",
      "Accuracy for Tranformer 1: 80.95057034220532 %\n",
      "Accuracy for Tranformer 2: 81.14068441064639 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Facenet 1:\", evaluate_model(pred_prob1),\"%\")\n",
    "print(\"Accuracy for Facenet 2:\", evaluate_model(pred_prob2),\"%\")\n",
    "print(\"Accuracy for Tranformer 1:\", evaluate_model(pred_prob3),\"%\")\n",
    "print(\"Accuracy for Tranformer 2:\", evaluate_model(pred_prob4),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b740078f",
   "metadata": {},
   "source": [
    "### Ensemble models accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50ef5e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Facenet ensemble: 93.8022813688213 %\n",
      "Accuracy for Tranformer ensemble: 88.8212927756654 %\n",
      "Accuracy for complete ensemble: 96.00760456273764 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Facenet ensemble:\", evaluate_model(pred_prob1+pred_prob2),\"%\")\n",
    "print(\"Accuracy for Tranformer ensemble:\", evaluate_model(pred_prob3+pred_prob4),\"%\")\n",
    "print(\"Accuracy for complete ensemble:\", evaluate_model(pred_prob1+pred_prob2+pred_prob3+pred_prob4),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d4d75",
   "metadata": {},
   "source": [
    "## For test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e74fcc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train2, x_test2, y_train, y_test = train_test_split(X,Y, test_size=0.2,shuffle=True, random_state=749)\n",
    "pred_prob1=Facenet1.predict(x_test2)\n",
    "pred_prob2=Facenet2.predict(x_test2)\n",
    "pred_prob3=layer12(transformerClassifier1.predict(x_test2)).numpy()\n",
    "pred_prob4=layer12(transformerClassifier2.predict(x_test2)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ab83a",
   "metadata": {},
   "source": [
    "### Standalone models accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8067f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Facenet 1: 92.33840304182509 %\n",
      "Accuracy for Facenet 2: 91.73003802281369 %\n",
      "Accuracy for Tranformer 1: 87.52851711026615 %\n",
      "Accuracy for Tranformer 2: 87.6235741444867 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Facenet 1:\", evaluate_model(pred_prob1),\"%\")\n",
    "print(\"Accuracy for Facenet 2:\", evaluate_model(pred_prob2),\"%\")\n",
    "print(\"Accuracy for Tranformer 1:\", evaluate_model(pred_prob3),\"%\")\n",
    "print(\"Accuracy for Tranformer 2:\", evaluate_model(pred_prob4),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c995d3b6",
   "metadata": {},
   "source": [
    "### Ensemble models accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "acaf7cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Facenet ensemble: 96.7490494296578 %\n",
      "Accuracy for Tranformer ensemble: 93.78326996197718 %\n",
      "Accuracy for complete ensemble: 97.98479087452472 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Facenet ensemble:\", evaluate_model(pred_prob1+pred_prob2),\"%\")\n",
    "print(\"Accuracy for Tranformer ensemble:\", evaluate_model(pred_prob3+pred_prob4),\"%\")\n",
    "print(\"Accuracy for complete ensemble:\", evaluate_model(pred_prob1+pred_prob2+pred_prob3+pred_prob4),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a5838",
   "metadata": {},
   "source": [
    "## For test_size = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89470595",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train3, x_test3, y_train, y_test = train_test_split(X,Y, test_size=0.02,shuffle=True, random_state=463)\n",
    "pred_prob1=Facenet1.predict(x_test3)\n",
    "pred_prob2=Facenet2.predict(x_test3)\n",
    "pred_prob3=layer12(transformerClassifier1.predict(x_test3)).numpy()\n",
    "pred_prob4=layer12(transformerClassifier2.predict(x_test3)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc70a0",
   "metadata": {},
   "source": [
    "### Standalone models accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fafc6620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Facenet 1: 95.24714828897338 %\n",
      "Accuracy for Facenet 2: 92.39543726235742 %\n",
      "Accuracy for Tranformer 1: 92.01520912547528 %\n",
      "Accuracy for Tranformer 2: 91.82509505703422 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Facenet 1:\", evaluate_model(pred_prob1),\"%\")\n",
    "print(\"Accuracy for Facenet 2:\", evaluate_model(pred_prob2),\"%\")\n",
    "print(\"Accuracy for Tranformer 1:\", evaluate_model(pred_prob3),\"%\")\n",
    "print(\"Accuracy for Tranformer 2:\", evaluate_model(pred_prob4),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fa3f14",
   "metadata": {},
   "source": [
    "### Ensemble models accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78f27bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Facenet ensemble: 98.28897338403041 %\n",
      "Accuracy for Tranformer ensemble: 97.14828897338404 %\n",
      "Accuracy for complete ensemble: 99.42965779467681 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Facenet ensemble:\", evaluate_model(pred_prob1+pred_prob2),\"%\")\n",
    "print(\"Accuracy for Tranformer ensemble:\", evaluate_model(pred_prob3+pred_prob4),\"%\")\n",
    "print(\"Accuracy for complete ensemble:\", evaluate_model(pred_prob1+pred_prob2+pred_prob3+pred_prob4),\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
